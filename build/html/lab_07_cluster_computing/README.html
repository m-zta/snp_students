
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta content="metadata description" lang="en" name="description" xml:lang="en" />
<meta content="description des métadonnées" lang="fr" name="description" xml:lang="fr" />
<meta content="Sphinx, MyST" name="keywords" />
<meta content="en_US" property="og:locale" />

    <title>07a - Cluster Computing &#8212; MPC Labs  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Ultra96-V2" href="../appendix/appendix_u96.html" />
    <link rel="prev" title="06 - Real-Time with SIMD" href="../lab_06_Real-Time_with_SIMD_Open_MP/README.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="a-cluster-computing">
<h1>07a - Cluster Computing<a class="headerlink" href="#a-cluster-computing" title="Permalink to this headline">¶</a></h1>
<section id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Message Passing Interface (MPI) really comes into its own in scientific computing. In cluster systems using tightly bound multiple CPUs and/or GPUs or in networked loosely bound computing centres, MPI supports computation complexity and massively parallel systems. The usefulness of the MPI environment is that it is interface agnostic. It can run on a local multicore or on a high performance computing machine made up of distributed computing machines.</p>
<p>As a simple example, we want to calculate the value of PI in a floating-point approximation on a cluster of Raspberry Pi boards. Using such a cluster we can demonstrate MPI parallelisation and OpenMP parallelisation. A cluster of commercial computers networked together to form a massed array of computing power is known as a Beowulf cluster.</p>
<p>There are two lab sessions dedicated to this laboratory. The laboratory will be marked. Group work is highly recommended.</p>
<section id="calculating-pi-on-a-cluster">
<h3>1.1 Calculating PI on a cluster<a class="headerlink" href="#calculating-pi-on-a-cluster" title="Permalink to this headline">¶</a></h3>
<p>There are several reasons to calculate PI. One is to provide an example of scientific and parallelisable computing of mathematical numbers. A second reason is to find algorithms for efficient computing of numbers to large numbers of decimal places. A third reason is to serve as a benchmark for parallel programs and architectures.</p>
<p>For the purposes of this lab, we want to calculate PI on a computing cluster of four Raspberry Pi boards connected in a LAN. The calculation of PI will be distributed among three computing nodes and one node serving as coordinator. Therefore, we want to parallelise the execution on four individual processes – each on one Raspberry Pi board of the cluster. The four processes will be executed by one <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> runtime.</p>
</section>
<section id="cluster-architecture">
<h3>1.2 Cluster architecture<a class="headerlink" href="#cluster-architecture" title="Permalink to this headline">¶</a></h3>
<p><img alt="cluster_architecture.png" src="../_images/cluster_architecture.png" /></p>
<p>The number PI can be approximated using the following integral:</p>
<p><img alt="pi_integral_formula.png" src="../_images/pi_integral_formula.png" /></p>
<p>Numerically, the integral is determined by dividing the curve under the area f(x) into N rectangles of the area Fi and summing up the partial areas:</p>
<p><img alt="pi_integral_curve.png" src="../_images/pi_integral_curve.png" /></p>
<p>The following formula can be used implementing an approximation of PI:</p>
<p><img alt="pi_integral_approximation.png" src="../_images/pi_integral_approximation.png" /></p>
</section>
</section>
<section id="learning-aims">
<h2>2. Learning Aims<a class="headerlink" href="#learning-aims" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>First steps with scientific high performance computing (HPC) computing</p></li>
<li><p>First steps with MPI</p></li>
<li><p>Handling a Beowulf cluster</p></li>
<li><p>Calcualtion-time differences on different architectures</p></li>
</ol>
</section>
<section id="task">
<h2>3. Task<a class="headerlink" href="#task" title="Permalink to this headline">¶</a></h2>
<section id="setup-the-cluster-of-raspberry-pi-boards">
<h3>3.1 Setup the cluster of Raspberry Pi boards<a class="headerlink" href="#setup-the-cluster-of-raspberry-pi-boards" title="Permalink to this headline">¶</a></h3>
<p>Follow these steps:</p>
<ol class="arabic simple">
<li><p>Setup the cluster of Raspberry Pi boards according to the instructions given during the lab session. In the end, it should look like in the picture below (where one ethernet cable is connected to your personal laptop and the cluster blade not yet connected to power):
<img alt="HW_setup.png" src="../_images/HW_setup.png" /></p></li>
<li><p>Flash the images provided on jupiter, the FTP server from previous labs, on your four SD cards (same procedure as with the U96 board). The image mpc_rpi_head_fs2022.img will be used once for the head node, the image mpc_rpi_child_fs2022.img will be used three times for each computing node.</p></li>
<li><p>After flashing the images on your SD cards, insert them into your Raspberry Pi boards and connect the cluster blade to the power source. The Raspberry Pi boards should now boot.</p></li>
</ol>
</section>
<section id="setup-the-lan">
<h3>3.2 Setup the LAN<a class="headerlink" href="#setup-the-lan" title="Permalink to this headline">¶</a></h3>
<p>Now we will set up the network. The head node has the static IP <em>192.168.42.100</em> which was defined beforehand in the file <em>/etc/network/interfaces</em>. The computing nodes will receive an IP address from the head node via DHCP. On the head node, a software package named <em>dnsmasq</em> was installed prior. This package runs as service which is started automatically during the boot process. This service includes a DHCP server which is responsible for the address distribution. Since every Raspberry Pi board is in the same subnet, we can retrieve the other IP addresses of each of the boards serving as computing nodes from the <em>leases</em> file of the DHCP server.</p>
<p>In a second step, we will give each node we want to communicate with in the network a dedicated name, i.e. every Raspberry Pi board is assigned a hostname. This is also helpful when we don’t want to type in the nodes’ IP addresses every time. The <em>hosts</em> file is provided by the device’s operating system and used to map hostnames to IP addresses. <strong>Note, we need to configure the hosts file of each board with the assigned names and mapping them to the correct IP addresses.</strong></p>
<p>Follow these steps:</p>
<ol class="arabic">
<li><p>Open a terminal and connect to your Raspberry Pi which serves as head node via SSH (Use the standard password “raspberry” to log in and answer the security question regarding authenticity with “yes”):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh pi@192.168.42.100
</pre></div>
</div>
</li>
<li><p>Retrieve the IP addresses of the other boards (and write them down somewhere):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat dnsmasq.leases
</pre></div>
</div>
</li>
<li><p>Edit the host file /etc/hosts and add the hostname mapping to the IP addresses you just retreived:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#MPI Cluster Setup</span>
<span class="m">192</span>.168.42.100		head-node
<span class="s2">&quot;IP_of_Pi1&quot;</span>		computing-node-0
<span class="s2">&quot;IP_of_Pi2&quot;</span>		computing-node-1
<span class="s2">&quot;IP_of_Pi3&quot;</span>		computing-node-2
</pre></div>
</div>
<p>These hostnames will be later used for mpirun.</p>
</li>
<li><p>Open three more terminals and connect to the other boards via SSH, using the retrieved IPs from above.</p></li>
<li><p>For each computing node, edit the host file /etc/hosts and add the hostname mapping to the IP addresses. Here we will need the mapping for the head node and the mapping for the current computing node.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#MPI Cluster Setup</span>
 <span class="m">192</span>.168.42.100		head-node
 <span class="s2">&quot;IP_of_PiX&quot;</span>		computing-node-X
</pre></div>
</div>
</li>
</ol>
</section>
<section id="setting-up-ssh">
<h3>3.3 Setting up SSH<a class="headerlink" href="#setting-up-ssh" title="Permalink to this headline">¶</a></h3>
<p>Our nodes will be communicating over the network via SSH. We will establish passwordless SSH in order to enable an easier login and communication between the nodes.</p>
<p>Follow these steps:</p>
<p>On the head node:</p>
<ol class="arabic">
<li><p>Generate keys, in our case RSA keys:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh-keygen -t rsa
</pre></div>
</div>
<p>(In the prompt, just press enter, no need to specify a file or passphrase.)</p>
</li>
<li><p>Copy the keys to the other nodes’ list of authorized_keys and connect to each computing node via ssh:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh-copy-id -i /home/pi/.ssh/id_rsa pi@<span class="s2">&quot;IP_of_PiX&quot;</span>
ssh@<span class="s2">&quot;IP_of_PiX&quot;</span>
<span class="nb">exit</span>
</pre></div>
</div>
<p>(Note, we are still on the head node,
repeat each step for every computing node X.)</p>
</li>
<li><p>For the communication to work with the assigned hostnames, you need to execute the following commands for each computing node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh computing-node-X
<span class="nb">exit</span>
</pre></div>
</div>
</li>
</ol>
<p>On each of the computing nodes:</p>
<ol class="arabic">
<li><p>Generate keys, in our case RSA keys:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh-keygen -t rsa
</pre></div>
</div>
<p>(In the prompt, just press enter, no need to specify a file or passphrase.)</p>
</li>
<li><p>Copy the keys to the head node’s list of authorized_keys and connect to the head node via ssh:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh-copy-id -i /home/pi/.ssh/id_rsa pi@192.168.42.100
ssh@192.168.42.100
<span class="nb">exit</span>
</pre></div>
</div>
<p><strong>(Note, repeat these steps on every computing node X.)</strong></p>
</li>
</ol>
</section>
<section id="shared-directory-via-nfs">
<h3>3.4 Shared directory via NFS<a class="headerlink" href="#shared-directory-via-nfs" title="Permalink to this headline">¶</a></h3>
<p>We will use a shared directory via NFS in the head node to exchange data which each computing node mounts. The directory is called mpc_cloud and will be used for the MPI application that runs on every node on the cluster. This shared directory via NFS has already been set up on the head node. However, we need to check if it is in fact mounted on each computing node. It is possible, that a computing node was not able to mount due to the simultaneous booting of all Raspberry Pi boards in the cluster blade. In this case, the head node was not able to start the NFS-kernel-server in advance.</p>
<p>Follow these steps:</p>
<ol class="arabic">
<li><p>Check if mount was successful:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>df -h
</pre></div>
</div>
</li>
<li><p>If you do not see 192.169.42.100:/home/pi/mpc_cloud in the list, execute the following command to mount:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo mount -t nfs <span class="m">192</span>.168.42.100:/home/pi/mpc_cloud 	/home/pi/mpc_cloud
</pre></div>
</div>
</li>
<li><p>If successful, you should see it now in:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>df -h
</pre></div>
</div>
</li>
<li><p>Check if you have the same contents in the folder mpc_cloud on the head node and on each of the computing nodes.</p></li>
</ol>
</section>
<section id="calculate-pi-on-a-cluster-of-pis">
<h3>3.5 Calculate PI on a cluster of Pis<a class="headerlink" href="#calculate-pi-on-a-cluster-of-pis" title="Permalink to this headline">¶</a></h3>
<p>When we are all set up, we want to calculate PI on our cluster of Raspberry Pi boards. MPI is used for distributing the calculation among the boards and for the communication between them. OpenMP is used to parallelise the calculation within each calculating node.</p>
<p>Follow these steps:</p>
<ol class="arabic">
<li><p>On the head node, switch to the folder <em>mpc_cloud</em> and edit the <em>cluster_cpmputing_template.c</em> file according to the following steps.</p></li>
<li><p>Implement the calculation of PI in OpenMP with the formula above using the template given. <strong>(See <em>calc_subsum_pi</em>(…) for instructions.)</strong></p></li>
<li><p>Fill in where necessary the MPI communication for the cluster network.</p></li>
<li><p>Compile as known and don’t forget to add <em>-fopenmp</em>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpicc -o cluster_computing cluster_computing_template.c -fopenmp
</pre></div>
</div>
</li>
<li><p>Execute <em>mpirun</em> with the previously determined hostnames.
Note the value calculated and the process time.</p></li>
<li><p>Change the number of rectangles used to calculate the total area beneath the curve. Use several different values. How does it affect the approximated value of PI and why?</p></li>
</ol>
</section>
</section>
<section id="evaluation">
<h2>4. Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h2>
<p>This laboratory is evaluated on the basis of the discussions with you on your code/solution. You can receive a maximum of 5 marks for this laboratory.</p>
<p>A perfect solution is good.</p>
<p>I <strong>welcome</strong> in-depth discussions on the contents of the lab and your solutions <strong>during</strong> the implementation process.</p>
</section>
<section id="version">
<h2>5. Version<a class="headerlink" href="#version" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-left head"><p>Version</p></th>
<th class="text-left head"><p>Date</p></th>
<th class="text-left head"><p>By</p></th>
<th class="text-left head"><p>Comments</p></th>
<th class="text-left head"><p>Class Level</p></th>
<th class="text-left head"><p>Module</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>V1.1</p></td>
<td class="text-left"><p>05.2022</p></td>
<td class="text-left"><p>donn</p></td>
<td class="text-left"><p>Ported to Pi-Blades</p></td>
<td class="text-left"><p>6S</p></td>
<td class="text-left"><p>MPC</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>V1.0</p></td>
<td class="text-left"><p>02.2021</p></td>
<td class="text-left"><p>donn</p></td>
<td class="text-left"><p>First version</p></td>
<td class="text-left"><p>6S</p></td>
<td class="text-left"><p>MPC</p></td>
</tr>
</tbody>
</table>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../index.html">
    <img class="logo" src="../_static/logo.png" alt="Logo"/>
    
    <h1 class="logo logo-name">MPC Labs</h1>
    
  </a>
</p>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../lab_01_Setup_Laboratory/README.html">01 - Setup Laboratory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lab_02_Process_Threads_Task_Decomposition/README.html">02 - Process, Threads, Task Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lab_03_Data_Decomposition/README.html">03 - Data Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lab_04_OpenMP/README.html">04 - OpenMP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lab_05_SIMD/README.html">05 - Incremental computation of moments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lab_06_Real-Time_with_SIMD_Open_MP/README.html">06 - Real-Time with SIMD</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">07a - Cluster Computing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">1. Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#calculating-pi-on-a-cluster">1.1 Calculating PI on a cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cluster-architecture">1.2 Cluster architecture</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#learning-aims">2. Learning Aims</a></li>
<li class="toctree-l2"><a class="reference internal" href="#task">3. Task</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#setup-the-cluster-of-raspberry-pi-boards">3.1 Setup the cluster of Raspberry Pi boards</a></li>
<li class="toctree-l3"><a class="reference internal" href="#setup-the-lan">3.2 Setup the LAN</a></li>
<li class="toctree-l3"><a class="reference internal" href="#setting-up-ssh">3.3 Setting up SSH</a></li>
<li class="toctree-l3"><a class="reference internal" href="#shared-directory-via-nfs">3.4 Shared directory via NFS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#calculate-pi-on-a-cluster-of-pis">3.5 Calculate PI on a cluster of Pis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation">4. Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#version">5. Version</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/appendix_u96.html">Ultra96-V2</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../lab_06_Real-Time_with_SIMD_Open_MP/README.html" title="previous chapter">06 - Real-Time with SIMD</a></li>
      <li>Next: <a href="../appendix/appendix_u96.html" title="next chapter">Ultra96-V2</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, stsh.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.3.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/lab_07_cluster_computing/README.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>